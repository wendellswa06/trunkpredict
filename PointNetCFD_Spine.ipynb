{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQOikECbKSPR"
      },
      "source": [
        "# 1. TensorFlow Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ccb-_8sZ6f2Z"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGyw0EmlIMcT"
      },
      "source": [
        "# 2. Importing libraries\n",
        "As a first step, we import the necessary libraries. We use [Matplotlib](https://matplotlib.org/) for visualization purposes and [NumPy](https://numpy.org/) for computing on arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0GgDaACIeuC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import linecache\n",
        "import math\n",
        "from operator import itemgetter\n",
        "import numpy as np\n",
        "from numpy import zeros\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['font.family'] = 'serif'\n",
        "plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif']\n",
        "plt.rcParams['font.size'] = '12'\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Reshape\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Convolution1D, MaxPooling1D\n",
        "from tensorflow.keras.layers import Lambda, concatenate\n",
        "\n",
        "# from tensorflow_graphics.math.interpolation.trilinear import interpolate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3 Pre-training with spine data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Data = np.load('Data_spine.npy')\n",
        "data_number = Data.shape[0]\n",
        "# print(Data)\n",
        "print('Number of data is:')\n",
        "print(data_number, Data.shape[1], Data.shape[2])\n",
        "print(\"///////////\")\n",
        "# print(len(Data[0]))\n",
        "print(\"/////////////\")\n",
        "# print(len(Data[0][0]))\n",
        "point_numbers = 5000\n",
        "\n",
        "space_variable = 3 # 2 in 2D (x,y) and 3 in 3D (x,y,z)\n",
        "cfd_variable = 3\n",
        "\n",
        "input_data = zeros([data_number,point_numbers,space_variable],dtype='f')\n",
        "output_data = zeros([data_number,point_numbers,cfd_variable],dtype='f')\n",
        "\n",
        "for i in range(data_number):\n",
        "    input_data[i,0:108,0] = Data[i,:,0] # x coordinate (m)\n",
        "    input_data[i,0:108,1] = Data[i,:,1] # y coordinate (m)z\n",
        "    input_data[i,0:108,2] = Data[i,:,2] # z coordinate (m)\n",
        "    \n",
        "    output_data[i,0:108,0] = Data[i,:,3] # u (m)\n",
        "    output_data[i,0:108,1] = Data[i,:,4] # v (m)\n",
        "    output_data[i,0:108,2] = Data[i,:,5] # p (m)\n",
        "            \n",
        "# print(input_data[0])\n",
        "# print(output_data[0])\n",
        "# print(input_data.shape[2])\n",
        "# print(output_data.shape[1])\n",
        "Root_Dir = os.path.abspath('')\n",
        "visual_dir = os.path.join(Root_Dir, \"predict_spine\")\n",
        "print(visual_dir)\n",
        "if not os.path.exists(visual_dir):\n",
        "    os.mkdir(visual_dir)\n",
        "\n",
        "\n",
        "fout = open(os.path.join(visual_dir, \"8_pred_spine.obj\"), 'w')\n",
        "for kk in range(point_numbers):\n",
        "    fout.write(f\"v {output_data[0][kk][0]} {output_data[0][kk][1]} {output_data[0][kk][2]}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "u_min = np.min(output_data[:,:,0])\n",
        "u_max = np.max(output_data[:,:,0])\n",
        "v_min = np.min(output_data[:,:,1])\n",
        "v_max = np.max(output_data[:,:,1])\n",
        "p_min = np.min(output_data[:,:,2])\n",
        "p_max = np.max(output_data[:,:,2])\n",
        "\n",
        "output_data[:,:,0] = (output_data[:,:,0] - u_min)/(u_max - u_min)\n",
        "output_data[:,:,1] = (output_data[:,:,1] - v_min)/(v_max - v_min)\n",
        "output_data[:,:,2] = (output_data[:,:,2] - p_min)/(p_max - p_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "indices = np.random.permutation(input_data.shape[0])\n",
        "training_idx, validation_idx, test_idx = indices[:int(0.8*data_number)], indices[int(0.8*data_number):int(0.9*data_number)], indices[int(0.9*data_number):]\n",
        "input_training, input_validation, input_test = input_data[training_idx,:], input_data[validation_idx,:], input_data[test_idx,:]\n",
        "output_training, output_validation, output_test = output_data[training_idx,:], output_data[validation_idx,:], output_data[test_idx,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaling = 0.25 #reasonable choices for scaling: 4.0, 2.0, 1.0, 0.25, 0.125\n",
        "\n",
        "def exp_dim(global_feature, num_points):\n",
        "    return tf.tile(global_feature, [1, num_points, 1])\n",
        "\n",
        "PointNet_input = Input(shape=(point_numbers, space_variable))#point_numbers=10000, space_variable=3\n",
        "#Shared MLP (64,64)\n",
        "branch1 = Convolution1D(int(64*scaling),1,input_shape=(point_numbers,space_variable), activation='relu')(PointNet_input)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "branch1 = Convolution1D(int(64*scaling),1,input_shape=(point_numbers,space_variable), activation='relu')(branch1)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "Local_Feature = branch1\n",
        "#Shared MLP (64,128,1024)\n",
        "branch1 = Convolution1D(int(64*scaling),1,activation='relu')(branch1)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "branch1 = Convolution1D(int(128*scaling),1,activation='relu')(branch1)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "branch1 = Convolution1D(int(1024*scaling),1,activation='relu')(branch1)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "#Max function\n",
        "Global_Feature = MaxPooling1D(pool_size=int(point_numbers*scaling))(branch1)\n",
        "# Global_Feature = MaxPooling1D(pool_size=int(point_numbers*1))(branch1)\n",
        "Global_Feature = Lambda(exp_dim, arguments={'num_points':int(scaling*point_numbers)})(Global_Feature)\n",
        "branch2 = concatenate([Local_Feature, Global_Feature])\n",
        "#Shared MLP (512,256,128)\n",
        "branch2 = Convolution1D(int(512*scaling),1,activation='relu')(branch2)\n",
        "branch2 = BatchNormalization()(branch2)\n",
        "branch2 = Convolution1D(int(256*scaling),1,activation='relu')(branch2)\n",
        "branch2 = BatchNormalization()(branch2)\n",
        "branch2 = Convolution1D(int(128*scaling),1,activation='relu')(branch2)\n",
        "branch2 = BatchNormalization()(branch2)\n",
        "#Shared MLP (128, cfd_variable)\n",
        "branch2 = Convolution1D(int(128*scaling),1,activation='relu')(branch2)\n",
        "branch2 = BatchNormalization()(branch2)\n",
        "PointNet_output = Convolution1D(cfd_variable,1,activation='sigmoid')(branch2) #Please note that we use the sigmoid activation function in the last layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from tensorflow.keras import backend as K\n",
        "# def chamfer_distance_prev(y_true, y_pred):\n",
        "#     \"\"\"\n",
        "#     Compute the Chamfer Distance between two point clouds\n",
        "#     \"\"\"\n",
        "#     y_true = K.reshape(y_true, shape=(-1, 3))\n",
        "#     y_pred = K.reshape(y_pred, shape=(-1, 3))\n",
        "\n",
        "#     num_points_y_true = tf.shape(y_true)[0]\n",
        "#     num_points_y_pred = tf.shape(y_pred)[0]\n",
        "\n",
        "#     # Compute pairwise distance matrix\n",
        "#     r_true = K.sum(y_true * y_true, axis=1)\n",
        "#     r_true = K.reshape(r_true, [-1, 1])\n",
        "#     r_pred = K.sum(y_pred * y_pred, axis=1)\n",
        "#     r_pred = K.reshape(r_pred, [1, -1])\n",
        "\n",
        "#     D = r_true - 2 * K.dot(y_true, K.transpose(y_pred)) + r_pred\n",
        "\n",
        "#     # Compute the minimum distance for each point in y_true\n",
        "#     min_distance_true = K.min(D, axis=1)\n",
        "\n",
        "#     # Compute the minimum distance for each point in y_pred\n",
        "#     min_distance_pred = K.min(D, axis=0)\n",
        "\n",
        "#     return K.mean(min_distance_true) + K.mean(min_distance_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "def pairwise_l2_norm2_batch(y_true, y_pred):\n",
        "    y_true = K.reshape(y_true, shape=(-1, 3))\n",
        "    y_pred = K.reshape(y_pred, shape=(-1, 3))\n",
        "    \n",
        "    num_points_y_true = tf.shape(y_true)[0]\n",
        "    num_points_y_pred = tf.shape(y_pred)[0]\n",
        "\n",
        "    # Compute pairwise distance matrix\n",
        "    r_true = K.sum(y_true * y_true, axis=1)\n",
        "    r_true = K.reshape(r_true, [-1, 1])\n",
        "    r_pred = K.sum(y_pred * y_pred, axis=1)\n",
        "    r_pred = K.reshape(r_pred, [1, -1])\n",
        "\n",
        "    D = r_true - 2 * K.dot(y_true, K.transpose(y_pred)) + r_pred\n",
        "    return D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def chamfer_distance(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the Chamfer Distance between two point clouds\n",
        "    \"\"\"\n",
        "    k_near = 8\n",
        "    # calculate shape loss\n",
        "    D = pairwise_l2_norm2_batch(y_true, y_pred)\n",
        "    min_distance_true = K.min(D, axis=1)\n",
        "    min_distance_pred = K.min(D, axis=0)\n",
        "    shapeLoss = K.mean(min_distance_true) + K.mean(min_distance_pred)\n",
        "    # calculate density loss\n",
        "    D2 = pairwise_l2_norm2_batch(y_true, y_true)\n",
        "    knndis = tf.nn.top_k(tf.negative(D), k=k_near)\n",
        "    knndis2 = tf.nn.top_k(tf.negative(D2), k=k_near)\n",
        "    densityLoss = tf.reduce_mean(tf.abs(knndis.values - knndis2.values))\n",
        "    # Compute the minimum distance for each point in y_true\n",
        "    # min_distance_true = K.min(D, axis=1)\n",
        "    # data_loss = shapeLoss + densityLoss * 2\n",
        "    # Compute the minimum distance for each point in y_pred\n",
        "    # min_distance_pred = K.min(D, axis=0)\n",
        "    data_loss = shapeLoss + densityLoss * 0.2\n",
        "    # return K.mean(min_distance_true) + K.mean(min_distance_pred)\n",
        "    return data_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.0001\n",
        "decaying_rate = 0.0\n",
        "model = Model(inputs=PointNet_input,outputs=PointNet_output)\n",
        "\n",
        "# model.compile(optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=0.000001, decay=decaying_rate)\n",
        "#                    , loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=learning_rate,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=decaying_rate)\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=0.00001)\n",
        "# model.compile(optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "# model.compile(optimizer, loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
        "checkpoint = ModelCheckpoint('fine_tuned_model.h5', save_best_only = True)\n",
        "model.compile(optimizer, loss=chamfer_distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch = 4\n",
        "epoch_number = 4000\n",
        "results = model.fit(input_training,output_training,batch_size=batch,epochs=epoch_number,shuffle=True,verbose=1, validation_split=0.0, validation_data=(input_validation, output_validation), callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjMn5rBkLEK6"
      },
      "source": [
        "# 4. Fine-tuning with trunk point cloud\n",
        "For your convinient, we have already prepared data as a numpy array. The data for this specific test case is the spatial coordinates of the finite volume (or finite element) grids and the values of velocity and pressure fields on those grid points. The spatial coordinates are the input of PointNet and the velocity (in the *x* and *y* directions) and pressure fields are the output of PointNet. Here, our focus is on 3D cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq9m3nL6jYzY"
      },
      "outputs": [],
      "source": [
        "# Data = np.load('PointNetCFD/Data.npy')\n",
        "Data = np.load('Data.npy')\n",
        "# Data = np.load('_Data.npy')\n",
        "\n",
        "# Data = \n",
        "data_number = Data.shape[0]\n",
        "# print(Data)\n",
        "print('Number of data is:')\n",
        "print(data_number, Data.shape[1], Data.shape[2])\n",
        "print(\"///////////\")\n",
        "# print(len(Data[0]))\n",
        "print(\"/////////////\")\n",
        "# print(len(Data[0][0]))\n",
        "point_numbers = 10000\n",
        "space_variable = 3\n",
        "cfd_variable = 3\n",
        "\n",
        "input_data = zeros([data_number,point_numbers//2,space_variable],dtype='f')\n",
        "output_data = zeros([data_number,point_numbers//2,cfd_variable],dtype='f')\n",
        "\n",
        "for i in range(data_number):\n",
        "    cnt = 0\n",
        "    for kk in range(point_numbers):\n",
        "        if kk % 2 == 0:\n",
        "            # input_data[i,:,0] = Data[i,:,0] # x coordinate (m)\n",
        "            # input_data[i,:,1] = Data[i,:,1] # y coordinate (m)\n",
        "            # input_data[i,:,2] = Data[i,:,2] # z coordinate (m)\n",
        "\n",
        "            # output_data[i,:,0] = Data[i,:,4] # u (m)\n",
        "            # output_data[i,:,1] = Data[i,:,5] # v (m)\n",
        "            # output_data[i,:,2] = Data[i,:,3] # p (m)\n",
        "            input_data[i,cnt,0] = Data[i,kk,0] # x coordinate (m)\n",
        "            input_data[i,cnt,1] = Data[i,kk,1] # y coordinate (m)\n",
        "            input_data[i,cnt,2] = Data[i,kk,2] # z coordinate (m)\n",
        "\n",
        "            output_data[i,cnt,0] = Data[i,kk,4] # u (m)\n",
        "            output_data[i,cnt,1] = Data[i,kk,5] # v (m)\n",
        "            output_data[i,cnt,2] = Data[i,kk,3] # p (m)\n",
        "\n",
        "            cnt += 1\n",
        "    \n",
        "point_numbers = 10000 // 2\n",
        "# print(input_data[0])\n",
        "# print(output_data[0])\n",
        "print(input_data.shape[2])\n",
        "print(output_data.shape[1])\n",
        "Root_Dir = os.path.abspath('')\n",
        "visual_dir = os.path.join(Root_Dir, \"predict\")\n",
        "print(visual_dir)\n",
        "if not os.path.exists(visual_dir):\n",
        "    os.mkdir(visual_dir)\n",
        "\n",
        "\n",
        "fout = open(os.path.join(visual_dir, \"8_pred.obj\"), 'w')\n",
        "for kk in range(point_numbers):\n",
        "    fout.write(f\"v {output_data[0][kk][0]} {output_data[0][kk][1]} {output_data[0][kk][2]}\\n\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L4MQt14yGeZ"
      },
      "outputs": [],
      "source": [
        "u_min = np.min(output_data[:,:,0])\n",
        "u_max = np.max(output_data[:,:,0])\n",
        "v_min = np.min(output_data[:,:,1])\n",
        "v_max = np.max(output_data[:,:,1])\n",
        "p_min = np.min(output_data[:,:,2])\n",
        "p_max = np.max(output_data[:,:,2])\n",
        "\n",
        "output_data[:,:,0] = (output_data[:,:,0] - u_min)/(u_max - u_min)\n",
        "output_data[:,:,1] = (output_data[:,:,1] - v_min)/(v_max - v_min)\n",
        "output_data[:,:,2] = (output_data[:,:,2] - p_min)/(p_max - p_min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6QGYJTn30WJ"
      },
      "outputs": [],
      "source": [
        "def plot2DPointCloud(x_coord,y_coord,z_coord,file_name):   \n",
        "    plt.scatter(x_coord,y_coord,z_coord,s=2.5)\n",
        "    plt.xlabel('x (m)')\n",
        "    plt.ylabel('y (m)')\n",
        "    plt.zlabel('z (m)')\n",
        "    x_upper = np.max(x_coord) + 1\n",
        "    x_lower = np.min(x_coord) - 1\n",
        "    y_upper = np.max(y_coord) + 1\n",
        "    y_lower = np.min(y_coord) - 1\n",
        "    z_upper = np.max(z_coord) + 1\n",
        "    z_lower = np.min(z_coord) - 1\n",
        "    plt.xlim([x_lower, x_upper])\n",
        "    plt.ylim([y_lower, y_upper])\n",
        "    plt.zlim([z_lower, z_upper])\n",
        "    plt.gca().set_aspect('equal', adjustable='box')\n",
        "    plt.savefig(f'{file_name}.png', dpi=300)\n",
        "    #plt.savefig(file_name+'.eps') #You can use this line for saving figures in EPS format   \n",
        "    plt.clf()\n",
        "    #plt.show()\n",
        "\n",
        "def plotSolution(x_coord,y_coord,z_coord,solution,file_name,title):\n",
        "    plt.scatter(x_coord,y_coord,z_coord,s=2.5,c=solution,cmap='jet')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('x (m)')\n",
        "    plt.ylabel('y (m)')\n",
        "    plt.zlabel('z (m)')\n",
        "    x_upper = np.max(x_coord) + 1\n",
        "    x_lower = np.min(x_coord) - 1\n",
        "    y_upper = np.max(y_coord) + 1\n",
        "    y_lower = np.min(y_coord) - 1\n",
        "    z_upper = np.max(z_coord) + 1\n",
        "    z_lower = np.min(z_coord) - 1\n",
        "    plt.xlim([x_lower, x_upper])\n",
        "    plt.ylim([y_lower, y_upper])\n",
        "    plt.zlim([z_lower, z_upper])\n",
        "    plt.gca().set_aspect('equal', adjustable='box')\n",
        "    cbar= plt.colorbar()\n",
        "    #cbar.set_label(label, labelpad=+1)\n",
        "    plt.savefig(f'{file_name}.png', dpi=300)\n",
        "    #plt.savefig(file_name+'.eps') #You can use this line for saving figures in EPS format\n",
        "    plt.clf()\n",
        "    #plt.show()\n",
        "    \n",
        "\n",
        "number = 1 #It should be less than number of data ('data_number')\n",
        "plot2DPointCloud(input_data[number,:,0],input_data[number,:,1],input_data[number,:,2],'PointCloud')\n",
        "plotSolution(input_data[number,:,0],input_data[number,:,1],output_data[number,:,0],'u_velocity','normalized u (x-velocity component)')\n",
        "plotSolution(input_data[number,:,0],input_data[number,:,1],output_data[number,:,1],'v_velocity','normalized v (y-velocity component)')\n",
        "plotSolution(input_data[number,:,0],input_data[number,:,1],output_data[number,:,2],'pressure','normalized pressure')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLHlpEJEy0uN"
      },
      "outputs": [],
      "source": [
        "indices = np.random.permutation(input_data.shape[0])\n",
        "training_idx, validation_idx, test_idx = indices[:int(0.8*data_number)], indices[int(0.8*data_number):int(0.9*data_number)], indices[int(0.9*data_number):]\n",
        "input_training, input_validation, input_test = input_data[training_idx,:], input_data[validation_idx,:], input_data[test_idx,:]\n",
        "output_training, output_validation, output_test = output_data[training_idx,:], output_data[validation_idx,:], output_data[test_idx,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh8Dqo3rzAEY"
      },
      "outputs": [],
      "source": [
        "\n",
        "scaling = 0.25 #reasonable choices for scaling: 4.0, 2.0, 1.0, 0.25, 0.125\n",
        "\n",
        "def exp_dim(global_feature, num_points):\n",
        "    return tf.tile(global_feature, [1, num_points, 1])\n",
        "\n",
        "PointNet_input = Input(shape=(point_numbers, space_variable))#point_numbers=10000, space_variable=3\n",
        "#Shared MLP (64,64)\n",
        "branch1 = Convolution1D(int(64*scaling),1,input_shape=(point_numbers,space_variable), activation='relu')(PointNet_input)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "branch1 = Convolution1D(int(64*scaling),1,input_shape=(point_numbers,space_variable), activation='relu')(branch1)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "Local_Feature = branch1\n",
        "#Shared MLP (64,128,1024)\n",
        "branch1 = Convolution1D(int(64*scaling),1,activation='relu')(branch1)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "branch1 = Convolution1D(int(128*scaling),1,activation='relu')(branch1)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "branch1 = Convolution1D(int(1024*scaling),1,activation='relu')(branch1)\n",
        "branch1 = BatchNormalization()(branch1)\n",
        "#Max function\n",
        "# Global_Feature = MaxPooling1D(pool_size=int(point_numbers*scaling))(branch1)\n",
        "Global_Feature = MaxPooling1D(pool_size=int(point_numbers*scaling))(branch1)\n",
        "Global_Feature = Lambda(exp_dim, arguments={'num_points':int(point_numbers*scaling)})(Global_Feature)\n",
        "branch2 = concatenate([Local_Feature, Global_Feature])\n",
        "#Shared MLP (512,256,128)\n",
        "branch2 = Convolution1D(int(512*scaling),1,activation='relu')(branch2)\n",
        "branch2 = BatchNormalization()(branch2)\n",
        "branch2 = Convolution1D(int(256*scaling),1,activation='relu')(branch2)\n",
        "branch2 = BatchNormalization()(branch2)\n",
        "branch2 = Convolution1D(int(128*scaling),1,activation='relu')(branch2)\n",
        "branch2 = BatchNormalization()(branch2)\n",
        "#Shared MLP (128, cfd_variable)\n",
        "branch2 = Convolution1D(int(128*scaling),1,activation='relu')(branch2)\n",
        "branch2 = BatchNormalization()(branch2)\n",
        "PointNet_output = Convolution1D(cfd_variable,1,activation='sigmoid')(branch2) #Please note that we use the sigmoid activation function in the last layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "def pairwise_l2_norm2_batch(y_true, y_pred):\n",
        "    y_true = K.reshape(y_true, shape=(-1, 3))\n",
        "    y_pred = K.reshape(y_pred, shape=(-1, 3))\n",
        "    \n",
        "    num_points_y_true = tf.shape(y_true)[0]\n",
        "    num_points_y_pred = tf.shape(y_pred)[0]\n",
        "\n",
        "    # Compute pairwise distance matrix\n",
        "    r_true = K.sum(y_true * y_true, axis=1)\n",
        "    r_true = K.reshape(r_true, [-1, 1])\n",
        "    r_pred = K.sum(y_pred * y_pred, axis=1)\n",
        "    r_pred = K.reshape(r_pred, [1, -1])\n",
        "\n",
        "    D = r_true - 2 * K.dot(y_true, K.transpose(y_pred)) + r_pred\n",
        "    return D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def chamfer_distance(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the Chamfer Distance between two point clouds\n",
        "    \"\"\"\n",
        "    k_near = 8\n",
        "    # calculate shape loss\n",
        "    D = pairwise_l2_norm2_batch(y_true, y_pred)\n",
        "    min_distance_true = K.min(D, axis=1)\n",
        "    min_distance_pred = K.min(D, axis=0)\n",
        "    shapeLoss = K.mean(min_distance_true) + K.mean(min_distance_pred)\n",
        "    # calculate density loss\n",
        "    D2 = pairwise_l2_norm2_batch(y_true, y_true)\n",
        "    knndis = tf.nn.top_k(tf.negative(D), k=k_near)\n",
        "    knndis2 = tf.nn.top_k(tf.negative(D2), k=k_near)\n",
        "    densityLoss = tf.reduce_mean(tf.abs(knndis.values - knndis2.values))\n",
        "    # Compute the minimum distance for each point in y_true\n",
        "    # min_distance_true = K.min(D, axis=1)\n",
        "    # data_loss = shapeLoss + densityLoss * 2\n",
        "    # Compute the minimum distance for each point in y_pred\n",
        "    # min_distance_pred = K.min(D, axis=0)\n",
        "    data_loss = shapeLoss + densityLoss * 0.2\n",
        "    # return K.mean(min_distance_true) + K.mean(min_distance_pred)\n",
        "    return data_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6mcnpjDz1zo"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.0001\n",
        "decaying_rate = 0.0\n",
        "model = Model(inputs=PointNet_input,outputs=PointNet_output)\n",
        "\n",
        "# model.compile(optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=0.000001, decay=decaying_rate)\n",
        "#                    , loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=learning_rate,\n",
        "    decay_steps=10000,\n",
        "    decay_rate=decaying_rate)\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=0.00001)\n",
        "# model.compile(optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "# model.compile(optimizer, loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.MeanSquaredError()])\n",
        "\n",
        "checkpoint = ModelCheckpoint('fine_tuned_model.h5', save_best_only = True)\n",
        "model.compile(optimizer, loss=chamfer_distance)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9Tyj1Aaz7de"
      },
      "outputs": [],
      "source": [
        "batch = 4\n",
        "epoch_number = 500\n",
        "\n",
        "# pretrained_weights = tf.keras.models.load_model('fine_tuned_model.h5', custom_objects={'chamfer_distance':chamfer_distance})\n",
        "# model.set_weights(pretrained_weights.get_weights())\n",
        "results = model.fit(input_training,output_training,batch_size=batch,epochs=epoch_number,shuffle=True,verbose=1, validation_split=0.0, validation_data=(input_validation, output_validation), callbacks=[checkpoint])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy\n",
        "from scipy.spatial import KDTree\n",
        "def calculate_curvature(point_cloud, radius):\n",
        "    # Create KDTree for efficient nearest neighbor search\n",
        "    kdtree = KDTree(point_cloud)\n",
        "\n",
        "    curvatures = []\n",
        "\n",
        "    for point in point_cloud:\n",
        "        # Find nearest neighbors within the given radius\n",
        "        _, indices = kdtree.query(point, k=radius)\n",
        "\n",
        "        # Extract the neighborhood points\n",
        "        neighborhood = point_cloud[indices]\n",
        "\n",
        "        # Calculate the covariance matrix\n",
        "        covariance_matrix = np.cov(neighborhood.T)\n",
        "\n",
        "        # Calculate eigenvalues of the covariance matrix\n",
        "        eigenvalues, _ = np.linalg.eig(covariance_matrix)\n",
        "\n",
        "        # Sort eigenvalues in ascending order\n",
        "        eigenvalues.sort()\n",
        "\n",
        "        # Calculate curvature using the formula\n",
        "        curvature = (2 * eigenvalues[0]) / (eigenvalues[0] + eigenvalues[1])\n",
        "\n",
        "        curvatures.append(curvature)\n",
        "\n",
        "    return curvatures\n",
        "for i in range(output_test.shape[0]):\n",
        "    print(calculate_curvature(output_test[i], 8))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def curvature(points):\n",
        "    # Calculate the curvature of a 3D point cloud using TensorFlow\n",
        "    print(points.shape)\n",
        "    x, y, z = tf.split(points, num_or_size_splits=3, axis=2)\n",
        "    print(x.shape)\n",
        "    print(tf.concat([x, y, z], axis=2).shape)\n",
        "    dx_dt, dy_dt, dz_dt = tf.image.image_gradients(tf.reshape(tf.concat([x, y, z], axis=2), (5, 5000, 3, 1)))\n",
        "    ds_dt = tf.sqrt(dx_dt**2 + dy_dt**2 + dz_dt**2)\n",
        "    d2x_ds2, _ = tf.image.image_gradients(dx_dt / ds_dt)\n",
        "    _, d2y_ds2 = tf.image.image_gradients(dy_dt / ds_dt)\n",
        "    _, _, d2z_ds2 = tf.image.image_gradients(dz_dt / ds_dt)\n",
        "    curvature = (d2x_ds2 + d2y_ds2 + d2z_ds2) / ds_dt\n",
        "    return curvature\n",
        "print(curvature(output_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lth0TI-33USM"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import sys\n",
        "\n",
        "test_set_number = test_idx.size\n",
        "print(test_set_number)\n",
        "sample_point_cloud = zeros([1,point_numbers,space_variable],dtype='f')\n",
        "truth_point_cloud = zeros([point_numbers,cfd_variable],dtype='f')\n",
        "Root_Dir = os.path.abspath('')\n",
        "visual_dir = os.path.join(Root_Dir, \"predict\")\n",
        "print(visual_dir)\n",
        "if not os.path.exists(visual_dir):\n",
        "    os.mkdir(visual_dir)\n",
        "\n",
        "\n",
        "# for j in range(point_numbers):\n",
        "#         for i in range(space_variable):\n",
        "print(input_data.shape[0])\n",
        "# exit()\n",
        "for s in range(input_data.shape[0]):\n",
        "    for j, i in itertools.product(range(point_numbers), range(space_variable)):\n",
        "        # sample_point_cloud[0][j][i] = input_test[s][j][i]\n",
        "        sample_point_cloud[0][j][i] = input_data[s][j][i]\n",
        "\n",
        "    prediction = model.predict(sample_point_cloud, batch_size=None, verbose=0)\n",
        "    ########### calc point separation rate #########\n",
        "    # dist = pairwise_l2_norm2_batch(prediction, output_test[s])\n",
        "    # Unnormalized\n",
        "    prediction[0,:,0] = prediction[0,:,0]*(u_max - u_min) + u_min\n",
        "    prediction[0,:,1] = prediction[0,:,1]*(v_max - v_min) + v_min\n",
        "    prediction[0,:,2] = prediction[0,:,2]*(p_max - p_min) + p_min\n",
        "    # print(prediction.shape)\n",
        "    ###################show obj##################\n",
        "    fout = open(os.path.join(visual_dir, f\"{s}_pred.obj\"), 'w')\n",
        "    for kk in range(point_numbers):\n",
        "        # print(kk)\n",
        "        fout.write(f\"v {prediction[0,:,0][kk]} {prediction[0,:,1][kk]} {prediction[0,:,2][kk]}\\n\")\n",
        "    \n",
        "    #############################################\n",
        "    # output_test[s,:,0] = output_test[s,:,0]*(u_max - u_min) + u_min\n",
        "    # output_test[s,:,1] = output_test[s,:,1]*(v_max - v_min) + v_min\n",
        "    # output_test[s,:,2] = output_test[s,:,2]*(p_max - p_min) + p_min\n",
        "# exit()\n",
        "# print(tf.reduce_mean(calculate_curvature(prediction[0], 8)) - tf.reduce_mean(calculate_curvature(output_test[0], 8)))\n",
        "# exit()\n",
        "D = pairwise_l2_norm2_batch(prediction, output_test)\n",
        "min_distance_true = K.min(D, axis=1)\n",
        "min_distance_pred = K.min(D, axis=0)\n",
        "shapeLoss = K.mean(min_distance_true) + K.mean(min_distance_pred)\n",
        "sepRate = K.mean(min_distance_true) / (1 - 0)\n",
        "print(chamfer_distance(prediction, output_test), shapeLoss, sepRate)\n",
        "# curvature1 = curvature(prediction[0])\n",
        "# curvature2 = curvature(output_test[0])\n",
        "# curvature_diff = tf.reduce_mean(tf.abs(curvature1 - curvature2)).numpy()\n",
        "# print(\"Curvature difference:\", curvature_diff)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "PointNetCFD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
